#!/usr/bin/env python3
"""
üß† BRAIN TRAINER - Sistema de Aprendizado End-to-End
Modelo: Q-Learning Avan√ßado com Experience Replay
"""

import sqlite3
import json
import numpy as np
import pickle
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import random
from collections import deque
import math

# Configura√ß√£o
logger = logging.getLogger("BrainTrainer")
DB_NAME = 'sniper_brain.db'
MODEL_PATH = 'brain_models/q_learning_model.pkl'

class QLearningBrain:
    """
    Sistema de Q-Learning avan√ßado para trading
    """
    
    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.3):
        self.alpha = alpha  # Taxa de aprendizado
        self.gamma = gamma  # Fator de desconto
        self.epsilon = epsilon  # Explora√ß√£o vs Explora√ß√£o
        
        # Q-table: estado -> a√ß√£o -> valor
        self.q_table = {}
        
        # Experience replay
        self.memory = deque(maxlen=10000)
        
        # Estat√≠sticas
        self.training_stats = {
            'episodes': 0,
            'total_reward': 0,
            'wins': 0,
            'losses': 0,
            'last_update': None
        }
        
        # Carregar modelo se existir
        self.load_model()
    
    def _state_to_key(self, state: Dict) -> str:
        """Converte estado para chave da Q-table"""
        # Features principais para estado
        features = [
            state.get('pattern', 'UNKNOWN'),
            state.get('timeframe', '15m'),
            state.get('direction', 'NEUTRAL'),
            state.get('ai_confidence', 0),
            state.get('market_scenario', 5),
            state.get('btc_trend', 'NEUTRAL'),
            state.get('btcd_trend', 'NEUTRAL')
        ]
        return '|'.join(str(f) for f in features)
    
    def _get_actions(self) -> List[str]:
        """Lista de a√ß√µes poss√≠veis"""
        return ['ENTER_LONG', 'ENTER_SHORT', 'SKIP']
    
    def get_action(self, state: Dict) -> str:
        """
        Escolhe a√ß√£o baseada na pol√≠tica Œµ-greedy
        """
        state_key = self._state_to_key(state)
        
        # Inicializar estado se n√£o existir
        if state_key not in self.q_table:
            self.q_table[state_key] = {a: 0.0 for a in self._get_actions()}
        
        # Œµ-greedy: explorar ou explorar
        if random.random() < self.epsilon:
            # Explorar: a√ß√£o aleat√≥ria
            return random.choice(self._get_actions())
        else:
            # Explorar: melhor a√ß√£o
            q_values = self.q_table[state_key]
            return max(q_values, key=q_values.get)
    
    def update(self, state: Dict, action: str, reward: float, next_state: Dict, done: bool):
        """
        Atualiza Q-value usando f√≥rmula Q-Learning
        """
        state_key = self._state_to_key(state)
        next_state_key = self._state_to_key(next_state)
        
        # Inicializar se necess√°rio
        if state_key not in self.q_table:
            self.q_table[state_key] = {a: 0.0 for a in self._get_actions()}
        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = {a: 0.0 for a in self._get_actions()}
        
        # Valor Q atual
        current_q = self.q_table[state_key][action]
        
        # Melhor valor Q do pr√≥ximo estado
        if done:
            next_max_q = 0
        else:
            next_max_q = max(self.q_table[next_state_key].values())
        
        # F√≥rmula Q-Learning
        new_q = current_q + self.alpha * (reward + self.gamma * next_max_q - current_q)
        
        # Atualizar Q-table
        self.q_table[state_key][action] = new_q
        
        # Salvar experi√™ncia para replay
        self.memory.append((state_key, action, reward, next_state_key, done))
        
        # Atualizar estat√≠sticas
        self.training_stats['total_reward'] += reward
        if reward > 0:
            self.training_stats['wins'] += 1
        elif reward < 0:
            self.training_stats['losses'] += 1
        
        return new_q
    
    def experience_replay(self, batch_size=32):
        """
        Treina com experi√™ncias passadas (replay)
        """
        if len(self.memory) < batch_size:
            return
        
        batch = random.sample(list(self.memory), batch_size)
        
        for state_key, action, reward, next_state_key, done in batch:
            # Garantir que estados existam
            if state_key not in self.q_table:
                self.q_table[state_key] = {a: 0.0 for a in self._get_actions()}
            if next_state_key not in self.q_table:
                self.q_table[next_state_key] = {a: 0.0 for a in self._get_actions()}
            
            # Atualizar
            current_q = self.q_table[state_key][action]
            next_max_q = 0 if done else max(self.q_table[next_state_key].values())
            new_q = current_q + self.alpha * (reward + self.gamma * next_max_q - current_q)
            self.q_table[state_key][action] = new_q
    
    def calculate_reward(self, trade_result: Dict) -> float:
        """
        Calcula recompensa baseada no resultado do trade
        """
        profit_pct = trade_result.get('profit_pct', 0)
        duration_hours = trade_result.get('duration_hours', 1)
        
        # Recompensa base: profit percentual
        reward = profit_pct
        
        # Penalizar trades longos (oportunidade custo)
        if duration_hours > 24:
            reward -= 0.5
        
        # B√¥nus para trades r√°pidos e lucrativos
        if profit_pct > 2 and duration_hours < 6:
            reward += 1.0
        
        # Penalizar grandes drawdowns
        max_drawdown = trade_result.get('max_drawdown', 0)
        if max_drawdown > 5:
            reward -= 2.0
        
        return reward
    
    def save_model(self):
        """Salva modelo em disco"""
        try:
            import os
            os.makedirs('brain_models', exist_ok=True)
            
            model_data = {
                'q_table': self.q_table,
                'training_stats': self.training_stats,
                'alpha': self.alpha,
                'gamma': self.gamma,
                'epsilon': self.epsilon,
                'saved_at': datetime.now().isoformat()
            }
            
            with open(MODEL_PATH, 'wb') as f:
                pickle.dump(model_data, f)
            
            logger.info(f"‚úÖ Modelo salvo: {MODEL_PATH}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar modelo: {e}")
            return False
    
    def load_model(self):
        """Carrega modelo do disco"""
        try:
            import os
            if os.path.exists(MODEL_PATH):
                with open(MODEL_PATH, 'rb') as f:
                    model_data = pickle.load(f)
                
                self.q_table = model_data.get('q_table', {})
                self.training_stats = model_data.get('training_stats', self.training_stats)
                self.alpha = model_data.get('alpha', 0.1)
                self.gamma = model_data.get('gamma', 0.9)
                self.epsilon = model_data.get('epsilon', 0.3)
                
                logger.info(f"‚úÖ Modelo carregado: {len(self.q_table)} estados")
                return True
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
        
        return False
    
    def get_stats(self) -> Dict:
        """Retorna estat√≠sticas do modelo"""
        total_trades = self.training_stats['wins'] + self.training_stats['losses']
        win_rate = self.training_stats['wins'] / max(1, total_trades)
        avg_reward = self.training_stats['total_reward'] / max(1, self.training_stats['episodes'])
        
        return {
            'states': len(self.q_table),
            'memory_size': len(self.memory),
            'episodes': self.training_stats['episodes'],
            'total_reward': self.training_stats['total_reward'],
            'win_rate': win_rate,
            'avg_reward': avg_reward,
            'last_update': self.training_stats.get('last_update')
        }


class BrainTrainer:
    """
    Sistema principal de treinamento end-to-end
    """
    
    def __init__(self):
        self.brain = QLearningBrain()
        self.db_conn = None
        
    def connect_db(self):
        """Conecta ao database"""
        try:
            self.db_conn = sqlite3.connect(DB_NAME)
            logger.info(f"‚úÖ Conectado ao database: {DB_NAME}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erro ao conectar ao database: {e}")
            return False
    
    def get_training_data(self, limit=1000) -> List[Dict]:
        """
        Busca dados de treinamento do database
        """
        if not self.db_conn:
            self.connect_db()
        
        try:
            cursor = self.db_conn.cursor()
            
            # Buscar amostras com valida√ß√£o AI
            query = """
            SELECT 
                id, symbol, timeframe, timestamp_detection,
                pattern_detected, direction, ohlcv_json,
                ai_verdict, ai_confidence, ai_reasoning,
                status
            FROM raw_samples 
            WHERE ai_verdict IS NOT NULL 
            ORDER BY timestamp_detection DESC 
            LIMIT ?
            """
            
            cursor.execute(query, (limit,))
            rows = cursor.fetchall()
            
            training_data = []
            for row in rows:
                data = {
                    'id': row[0],
                    'symbol': row[1],
                    'timeframe': row[2],
                    'timestamp': row[3],
                    'pattern': row[4],
                    'direction': row[5],
                    'ohlcv': json.loads(row[6]) if row[6] else [],
                    'ai_verdict': row[7],
                    'ai_confidence': row[8] or 0,
                    'ai_reasoning': row[9],
                    'status': row[10]
                }
                training_data.append(data)
            
            logger.info(f"üìä {len(training_data)} amostras para treinamento")
            return training_data
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar dados: {e}")
            return []
    
    def extract_state_features(self, sample: Dict, market_context: Dict = None) -> Dict:
        """
        Extrai features do estado para o modelo
        """
        # Features b√°sicas do padr√£o
        state = {
            'pattern': sample.get('pattern', 'UNKNOWN'),
            'timeframe': sample.get('timeframe', '15m'),
            'direction': sample.get('direction', 'NEUTRAL'),
            'ai_confidence': sample.get('ai_confidence', 0),
            'symbol': sample.get('symbol', 'UNKNOWN')
        }
        
        # Adicionar contexto de mercado se dispon√≠vel
        if market_context:
            state.update({
                'market_scenario': market_context.get('scenario_number', 5),
                'btc_trend': market_context.get('btc_trend', 'NEUTRAL'),
                'btcd_trend': market_context.get('btcd_trend', 'NEUTRAL'),
                'btcd_source': market_context.get('btcd_source', 'unknown')
            })
        
        # Calcular features t√©cnicas se OHLCV dispon√≠vel
        ohlcv = sample.get('ohlcv', [])
        if len(ohlcv) >= 20:
            try:
                closes = [c[4] for c in ohlcv[-20:]]  # √öltimos 20 closes
                
                # M√©dias m√≥veis
                sma10 = sum(closes[-10:]) / 10 if len(closes) >= 10 else closes[-1]
                sma20 = sum(closes[-20:]) / 20
                
                state.update({
                    'price_trend': 'UP' if closes[-1] > sma10 else 'DOWN',
                    'volatility': (max(closes) - min(closes)) / min(closes) * 100,
                    'sma_distance': (closes[-1] - sma20) / sma20 * 100
                })
            except:
                pass
        
        return state
    
    def simulate_trade(self, sample: Dict, action: str) -> Dict:
        """
        Simula resultado do trade (para treinamento offline)
        """
        # Em produ√ß√£o real, isso viria do banco de dados de trades reais
        # Por enquanto, simulamos baseado na confian√ßa da AI
        
        ai_confidence = sample.get('ai_confidence', 0.5)
        direction = sample.get('direction', 'NEUTRAL')
        
        # Baseado em dados hist√≥ricos (aproxima√ß√£o)
        if action == 'SKIP':
            return {'profit_pct': 0, 'duration_hours': 0, 'max_drawdown': 0}
        
        # Verificar se a√ß√£o combina com dire√ß√£o
        action_matches = (
            (action == 'ENTER_LONG' and direction == 'LONG') or
            (action == 'ENTER_SHORT' and direction == 'SHORT')
        )
        
        if not action_matches:
            # Entrar contra a dire√ß√£o do padr√£o geralmente d√° preju√≠zo
            return {'profit_pct': -random.uniform(1, 5), 'duration_hours': random.uniform(2, 48), 'max_drawdown': random.uniform(3, 10)}
        
        # Simular resultado baseado na confian√ßa da AI
        if ai_confidence > 0.7:
            # Alta confian√ßa ‚Üí maior chance de lucro
            profit = random.uniform(0.5, 5.0) if random.random() < 0.7 else random.uniform(-2, -0.5)
        elif ai_confidence > 0.5:
            # M√©dia confian√ßa ‚Üí resultado misto
            profit = random.uniform(-1, 3) if random.random() < 0.6 else random.uniform(-3, -0.5)
        else:
            # Baixa confian√ßa ‚Üí maior chance de preju√≠zo
            profit = random.uniform(-3, 1) if random.random() < 0.4 else random.uniform(-5, -1)
        
        return {
            'profit_pct': profit,
            'duration_hours': random.uniform(1, 24),
            'max_drawdown': abs(profit * random.uniform(0.5, 2))
        }
    
    def train_offline(self, episodes=100):
        """
        Treinamento offline com dados hist√≥ricos
        """
        logger.info(f"üöÄ Iniciando treinamento offline ({episodes} epis√≥dios)")
        
        training_data = self.get_training_data(limit=500)
        if not training_data:
            logger.warning("‚ùå Nenhum dado para treinamento")
            return
        
        for episode in range(episodes):
            episode_reward = 0
            
            # Embaralhar dados
            random.shuffle(training_data)
            
            for i, sample in enumerate(training_data[:50]):  # Limitar por epis√≥dio
                # Extrair estado
                state = self.extract_state_features(sample)
                
                # Escolher a√ß√£o
                action = self.brain.get_action(state)
                
                # Simular resultado
                trade_result = self.simulate_trade(sample, action)
                reward = self.brain.calculate_reward(trade_result)
                
                # Pr√≥ximo estado (mesmo para simula√ß√£o)
                next_state = state  # Em simula√ß√£o simples
                
                # Atualizar Q-learning
                self.brain.update(state, action, reward, next_state, done=True)
                
                episode_reward += reward
            
            # Experience replay
            self.brain.experience_replay(batch_size=32)
            
            # Atualizar estat√≠sticas
            self.brain.training_stats['episodes'] += 1
            self.brain.training_stats['last_update'] = datetime.now().isoformat()
            
            # Salvar periodicamente
            if episode % 10 == 0:
                self.brain.save_model()
                logger.info(f"üìà Epis√≥dio {episode}: Recompensa = {episode_reward:.2f}")
        
        # Salvar modelo final
        self.brain.save_model()
        stats = self.brain.get_stats()
        logger.info(f"‚úÖ Treinamento completo: {stats}")
    
    def predict(self, state: Dict) -> Tuple[str, float]:
        """
        Predi√ß√£o em tempo real
        Retorna: (a√ß√£o, confian√ßa)
        """
        action = self.brain.get_action(state)
        
        # Calcular confian√ßa baseada nos Q-values
        state_key = self.brain._state_to_key(state)
        if state_key in self.brain.q_table:
            q_values = self.brain.q_table[state_key]
            max_q = max(q_values.values())
            min_q = min(q_values.values())
            
            if max_q == min_q:
                confidence = 0.5
            else:
                # Normalizar para 0-1
                confidence = (q_values[action] - min_q) / (max_q - min_q)
        else:
            confidence = 0.5  # Estado novo
        
        return action, confidence